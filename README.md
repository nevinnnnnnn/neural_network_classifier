# neural_network_classifier
This project showcases how to build a simple neural network classifier from scratch using NumPy, aimed at recognizing the letters A, B, and C from 5×6 pixel images. Instead of using high-level libraries like TensorFlow or PyTorch, the goal is to deeply understand the core mechanics of neural networks—such as forward and backward propagation, activation functions, and gradient descent—by implementing them manually. The dataset is generated by creating base patterns for each letter and adding random noise, resulting in 300 training samples and 30 test samples, with each label one-hot encoded. The neural network itself includes an input layer with 30 neurons (to handle the flattened image), a hidden layer of 10 neurons with sigmoid activation, and an output layer of 3 neurons with softmax to classify the letters. Training is done using cross-entropy loss and backpropagation, updating weights and biases over 1000 epochs with a learning rate of 0.1. Throughout training, loss and accuracy are tracked, and performance is later evaluated on unseen noisy test data. Visualizations include training progress, original letter patterns, and prediction results. Overall, the project serves as a hands-on, educational deep dive into how neural networks work under the hood, covering essential concepts like activation functions, gradient descent, and image classification—making it a great foundational exercise for anyone new to machine learning.
